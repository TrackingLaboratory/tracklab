_target_: tracklab.wrappers.DDSORT

_recursive_: False
training_enabled: True

train_cfg:
  use_wandb: ${use_wandb}
  use_rich: ${use_rich}
  evaluate_only: False   # activate to skip training and eval only
  pl_trainer:
    max_epochs: 25
    precision: 32
    gradient_clip_val: 0.1
    accumulate_grad_batches: 1  # 1 for no accumulation
    enable_progress_bar: True
    enable_model_summary: False
    profiler: null  # "simple" or "advanced", activate to display time profiling info
    num_sanity_val_steps: 0  # sanity run of val batches before training
    log_every_n_steps: 1  # default 50
    check_val_every_n_epochs: 1  # default 1
    val_check_interval: null  # default null
    fast_dev_run: False

ddsort:
  #sort:  # naive assoc()
  #  _target_: dd_sort.DDSORT
  #  _enabled_: True
  #  min_hits: 1  # number of hits to become a tracklet
  #  max_wo_hits: 50  # number of misses before removing from tracklet
  #  det_threshold: 0.1  # detection threshold
  bytetrack:  # bytetrack assoc()
    _target_: dd_sort.DDSORTBYTETracker
    _enabled_: True
    det_high_thresh: 0.4  # high detection threshold
    det_low_thresh: 0.1  # low detection threshold
    max_time_lost: 30  # number of misses before removing from tracklet
    simformer_call_strat: 3call  # {3call, 1call} 3call: call simformer 3 times, 1call: call simformer 1 time

det_filter_cfg:
  min_vis_keypoints: 3  # filter on inputs of the tracker
  vis_keypoint_threshold: 0.3  # filter on inputs of the tracker
  min_bbox_threshold: 0.1 # filter on inputs of the tracker

checkpoint_path: null  # overrides the checkpoint_paths from the submodules
override_cfg: null  # null if you do not want to override args from the checkpoints

simformer:
  _target_: dd_sort.SimFormer

  transformer_cfg:
    _target_: dd_sort.Encoder  # {Identity, Encoder, Decoder, Perceptron}
    emb_dim: 1024
    n_heads: 16
    n_layers: 1
    dim_feedforward: 4096
    dropout: 0.1
    activation_fn: gelu
    use_processed_track_tokens: True  # use the processed track tokens instead of the input ones
    src_key_padding_mask: True  # use the `src_key_padding_mask` in the transformer instead of 'src_mask'
    checkpoint_path: null  # override the weights of this module using this path

#  transformer_cfg:  # Perceptron to simply pass the merged tokens through a MLP
#    _target_: dd_sort.Perceptron  # {Identity, Encoder, Decoder, Perceptron}
#    checkpoint_path: null
#    emb_dim: 1024
#    n_layers: 2
#    dim_feedforward: 2048

  # Declare the list of tokenizers to use here. Tokenizer key is arbitrary
  tokenizers_cfg:
    #LinearAppearance:
    #  _target_: dd_sort.LinearAppearance
    #  _enabled_: True
    #  enable_ll: True
    #  token_dim: ${...transformer_cfg.emb_dim}
    #  feat_dim: 1799
    #  agg_strat: ema  # {ema, mean, last}
    #  checkpoint_path: null  # override the weights of this module using this path
    SmartLinearAppearance:
      _target_: dd_sort.SmartLinearAppearance
      _enabled_: True
      enable_ll: True
      checkpoint_path: null  # override the weights of this module using this path
      token_dim: ${...transformer_cfg.emb_dim}
      feat_dim: 3078

    MotionBertTokenizer:
      _target_: dd_sort.MotionBertTokenizer
      _enabled_: False
      checkpoint_path: ${model_dir}/MotionBERT/motionbert_42085620_still-dawn-1446.ckpt  # override the weights of this module using this path
      mb_checkpoint_path: ${model_dir}/MotionBERT/motionbert_lite.bin  # init weights for training
      freeze: False
      token_dim: ${...transformer_cfg.emb_dim}
      agg_strat: ema
      pad_empty_frames: True  # Will pad each empty frame with 0s if True, or use a concatenation of all available skeletons if False
      tracklet_max_age: 20  # Will use skeletons until this number of frames in the past, padding with 0s when necessary

  classifier_cfg:
    # null for no classifier
    #_target_: dd_sort.LinearClassifier
    #_enabled_: True
    #emb_dim: ${..transformer_cfg.emb_dim}
    #checkpoint_path: null  # override the weights of this module using this path
    _target_: dd_sort.MLPClassifier
    _enabled_: True
    checkpoint_path: null  # override the weights of this module using this path
    emb_dim: ${..transformer_cfg.emb_dim}
    hidden_dim: # must be a list that finishes with 1
      - 128
      - 1
    activation_fn: relu
    dropout: ${..transformer_cfg.dropout}

  merge_token_strat: sum
  sim_strat: cosine
  assos_strat: hungarian
  sim_threshold: 0.7  # sim_former similarity threshold for hungarian association (based on cosine distance of features). If 'null', threshold will be computed automatically
  tl_margin: 0.5  # margin for triplet loss
  loss_strat: triplet  # {triplet; infoNCE}
  contrastive_loss_strat: "valid_inter_intra"  # "inter": only det can be positive/negative of track and vice-versa, "inter_intra": det or track can be positive/negative of track and vice-versa
                                               # "valid_inter" or "valid_inter_intra": only valid det/track (track_ids >= 0)

  train_cfg: # AdamW
    init_lr: 1e-4
    weight_decay: 1e-2
    alpha_loss: 0.75

  transforms:
    train:
      _target_: dd_sort.simformer.transforms.Compose
      transforms:
        - _target_: dd_sort.simformer.transforms.AppMixup
          std: 0.005

datamodule:
  _target_: dd_sort.SimFormerDataModule
  dataset_splits: # should be a list containing train/val/test
    - train
    - val
  path: ${project_dir}/states # absolute path, otherwise it will be created for each job
  tracker_states:
    train: "${project_dir}/states/posetrack21-train.pklz"
    val: "${project_dir}/states/posetrack21-val.pklz"
    test: "${project_dir}/states/posetrack21-val.pklz"
  name: ddsort-posetrack  # name of the file for the gallery pickle
  batch_size: 8
  num_samples: 16
  num_videos: null # how many videos you want samples from, null for all
  samples_per_video: 50  # number of frame per video
  std_age: 5  # std of the random choice for the age
  max_length: 12  # max number of detections for a tracklet
  sampler: "harder"  # simple, hard or harder
  sampler_args:
    ids_bias: 10
    mix_frames: False
  transforms:
    - add_gt_reid_embeddings
    - add_gt_poses
    - normalize2image
    #- add_crops
    #- add_detections
    #- add_detections_with_id_switch
  num_workers: ${num_cores}
  online_transforms:
    train:
      _target_: dd_sort.simformer.transforms.Compose
      transforms:
        _target_: dd_sort.simformer.transforms.Compose
        transforms:
          - _target_: dd_sort.simformer.transforms.RandomAgeTracklet
            std_age: 5
          - _target_: dd_sort.simformer.transforms.RandomLengthTracklet
            max_length: 20
          - _target_: dd_sort.simformer.transforms.RandomGapsTracklet
            max_gap_size: 3
            max_gaps: 2
